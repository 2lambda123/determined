name: minimal_testing
workspace: DS_AT_PROTOTYPING
project: minimal_testing
max_restarts: 1
resources:
  slots_per_trial: 2
  max_slots: 8
  # resource_pool: young # Grenoble specific; remove later.
searcher:
  name: custom
  metric: throughput
  smaller_is_better: False
hyperparameters:
  data:
    root: /run/determined/workdir/datasets/CIFAR10 # Custom-master Grenoble specific.
  # NOTE: dsat code expects usual DS config dict to appear as in the below.
  ds_config:
    train_micro_batch_size_per_gpu: 128
    gradient_accumulation_steps: 1
    optimizer:
      TYPE: Adam
      params:
        lr: 1e-3
    fp16:
      enabled: False
    autotuning:
      enabled: True
      tuner_type: random
      tuner_num_trials: 50
      num_tuning_micro_batch_sizes: 3
      tuner_early_stopping: 5
      metric: throughput
    # Using torch_distributed's launcher for simplicity. TODO: Use DS launcher instead (?).
entrypoint: python3 -m determined.launch.torch_distributed python3 cifar10_script.py
